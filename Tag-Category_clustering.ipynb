{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet \n",
    "import wordsegment\n",
    "from wordsegment import segment\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '/Users/FQ/Downloads/finalDataset.csv'\n",
    "df1 = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = wordnet.synset('run.v.01') # v here denotes the tag verb \n",
    "w2 = wordnet.synset('sprint.v.01') \n",
    "print(w1.wup_similarity(w2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell returns error since hashtags can not be recognized by wordnet as they are not words. \n",
    "w1 = wordnet.synset(df1[\"Hashtag\"][1]+\".n.01\")\n",
    "w2 = wordnet.synset(df1[\"Hashtag\"][2]+\".n.01\")\n",
    "print(w1.wup_similarity(w2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all categories into memory\n",
    "cats = list(set(df1[\"Category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing &, /, , \n",
    "for i in range(len(cats)):\n",
    "    cats[i] = re.sub(r\"\\/\", \" \", cats[i])\n",
    "    cats[i] = re.sub(\"\\&\", \"\", cats[i])\n",
    "    cats[i] = re.sub(\"\\,\", \"\", cats[i])\n",
    "\n",
    "cats = [cat.lower() for cat in cats]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy download en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "#nlp = spacy.load('en_vectors_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization is different for upper and lower case\n",
    "w1 = nlp(\"TV Media Entertainment\")\n",
    "w2 = nlp(\"TV\")\n",
    "w3 = nlp(\"Media\")\n",
    "w4 = nlp(\"Entertainment\")\n",
    "vec1 = w1.vector\n",
    "vec2 = w2.vector\n",
    "vec3 = (w2.vector+w3.vector+w4.vector)\n",
    "print(\"distance between \\\"TV\\\" and \\\"TV Media Entertainment\\\":\",cosine_similarity(vec2.reshape(1,len(vec2)),vec1.reshape(1,len(vec1))))\n",
    "print(\"distance between \\\"TV\\\" and \\\"TV\\\"+ \\\"Media\\\"+ \\\"Entertainment\\\":\",cosine_similarity(vec2.reshape(1,len(vec2)),vec3.reshape(1,len(vec3))))\n",
    "w1 = nlp(\"TV Media Entertainment\".lower())\n",
    "w2 = nlp(\"TV\".lower())\n",
    "w3 = nlp(\"Media\".lower())\n",
    "w4 = nlp(\"Entertainment\".lower())\n",
    "vec1 = w1.vector\n",
    "vec2 = w2.vector\n",
    "vec3 = (w2.vector+w3.vector+w4.vector)\n",
    "print(\"distance between \\\"tv\\\" and \\\"tv media entertainment\\\":\",cosine_similarity(vec2.reshape(1,len(vec2)),vec1.reshape(1,len(vec1))))\n",
    "print(\"distance between \\\"tv\\\" and \\\"tv\\\"+ \\\"media\\\"+ \\\"entertainment\\\":\",cosine_similarity(vec2.reshape(1,len(vec2)),vec3.reshape(1,len(vec3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the lesson is in the case of compound word it is better to consider the sum of vectors\n",
    "split_cats = [cat.split(\" \") for cat in cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    vec = np.zeros(384)\n",
    "    for word in x:\n",
    "        if word!= \"\": \n",
    "            tok_word = nlp(word)\n",
    "            vec+=tok_word.vector  \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cats_vec = list(map(lambda x: f(x), split_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 30\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters= True)\n",
    "assigned_clusters = kclusterer.cluster(split_cats_vec, assign_clusters=True)\n",
    "print(assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = [[] for i in range(NUM_CLUSTERS)]\n",
    "for i, cls_num in enumerate(assigned_clusters):  \n",
    "    cls[cls_num].append(cats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cat={}\n",
    "for i in range(69):\n",
    "    for j in range(i+1,70):\n",
    "        u = split_cats_vec[i]\n",
    "        v = split_cats_vec[j]\n",
    "        dist = cosine_similarity(u.reshape(1,384),v.reshape(1,384))\n",
    "        if dist >= 0.8:\n",
    "            dict_cat[(i,j)] = 1\n",
    "        else:\n",
    "            dict_cat[(i,j)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l = np.where(np.array(list(dict_cat.values()))==1)\n",
    "keys = list(dict_cat.keys())\n",
    "for i in l[0]:\n",
    "    print(cats[keys[i][0]], \" \",cats[keys[i][1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordsegment import load, segment\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = df1[\"Hashtag\"]\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tags = list(map(lambda x: segment(x), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tags_vec = list(map(lambda x: f(x), split_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('split_tags_vec.npy', split_tags_vec)\n",
    "#tst = np.load('split_tags_vec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 30\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters= True)\n",
    "assigned_clusters = kclusterer.cluster(split_cats_vec, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_tags_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tag={}\n",
    "for i in range(63433):\n",
    "    for j in range(i+1,63434):\n",
    "        u = split_tags_vec[i]\n",
    "        v = split_tags_vec[j]\n",
    "        dist = cosine_similarity(u.reshape(1,384),v.reshape(1,384))\n",
    "        if dist >= 0.8:\n",
    "            dict_tag[(i,j)] = 1\n",
    "        else:\n",
    "            dict_tag[(i,j)] = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = split_tags_vec[38]\n",
    "v = split_tags_vec[20712]\n",
    "cosine_similarity(u.reshape(1,384),v.reshape(1,384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tags[38]\n",
    "split_tags[20712]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(dict_tag.keys())\n",
    "list(dict_tag.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.where(np.array(list(dict_tag.values()))==1)\n",
    "keys = list(dic.keys())\n",
    "for i in l[0]:\n",
    "    print(cats[keys[i][0]], \" \",cats[keys[i][1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=split_tags_vec[1]\n",
    "v=split_tags_vec[2]\n",
    "cosine_similarity(v.reshape(1,384),u.reshape(1,384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = [[] for i in range(NUM_CLUSTERS)]\n",
    "for i, cls_num in enumerate(assigned_clusters):  \n",
    "    cls[cls_num].append(df1[\"Category\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import collections\n",
    "d = df1.groupby(\"Category\")[\"Hashtag\"].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat2tag = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat2tag[\"Category\"] = ['Advertising', 'Agency', 'Animals', 'Apps', 'Art', 'Aviation', 'Barber',\n",
    "       'Beauty', 'Cars', 'Clothing', 'Craft', 'Ecommerce', 'Electronics',\n",
    "       'Events', 'Fashion', 'Feng shui', 'Film/ Cinema', 'Film/Cinema',\n",
    "       'Finance', 'Financial Services', 'Fishing', 'Fitness', 'Food',\n",
    "       'Food/ Resturants', 'Food/Resturants', 'FundRaising', 'Furniture',\n",
    "       'General', 'Home Decor', 'Influencer', 'Inspirational', 'Kids', 'LOVE',\n",
    "       'Legal/Law', 'Life Coaching', 'Luxury', 'Marketing', 'Medical',\n",
    "       'Modelling', 'Music', 'Nature', 'Nutrition', 'Painting', 'Personal',\n",
    "       'Pets', 'Photography/ Videography', 'Photography/Videography',\n",
    "       'PodCast', 'Poetry', 'Political', 'Psychology', 'RealEstate',\n",
    "       'Realstate', 'Religious', 'Small Business', 'Smartphone & Gadgets',\n",
    "       'Sports', 'TV, Media, Entertainment', 'Tattoos', 'Technology', 'Travel',\n",
    "       'Vaping', 'Wedding', 'Wine', 'Writing', 'Yoga', 'interieur', 'jewelry',\n",
    "       'meme', 'weddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat2tag[\"tags\"] = list(d.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat2tag[\"n_tags\"] = [len(x) for x in list(d.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-serving-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-serving-client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/Users/FQ/DownloadS/uncased_L-12_H-768_A-12/bert_config.json\", \"r\")\n",
    "print(f.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bert-serving-start -model_dir=\"/Users/FQ/Downloads/uncased_L-12_H-768_A-12/\" -num_worker=4 \n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "sent_encod = bc.encode(['I feel terrible', 'It is awesome day', 'I am sick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(sent_encod[0].reshape(1,-1),sent_encod[1].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '/Users/FQ/Downloads/finalDataset.csv'\n",
    "df1 = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(set(df1[\"Category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cats)):\n",
    "    cats[i] = re.sub(r\"\\/\", \" \", cats[i])\n",
    "    cats[i] = re.sub(\"\\&\", \"\", cats[i])\n",
    "    cats[i] = re.sub(\"\\,\", \"\", cats[i])\n",
    "\n",
    "cats = [cat.lower() for cat in cats]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_encod = bc.encode(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 35\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters= True)\n",
    "assigned_clusters = kclusterer.cluster(cats_encod, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = [[] for i in range(NUM_CLUSTERS)]\n",
    "for i, cls_num in enumerate(assigned_clusters):  \n",
    "    cls[cls_num].append(cats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
